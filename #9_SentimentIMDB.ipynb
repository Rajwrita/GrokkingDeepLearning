{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "#9_SentimentIMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariG23498/GrokkingDeepLearning/blob/master/9_SentimentIMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMf-nNpMuZRy",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bTDrGcmCMaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "690336a4-31d6-485f-b709-16ea3082f029"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_O6Cul4DJLk",
        "colab_type": "code",
        "outputId": "022a8026-36c7-4783-f689-8e386a24209d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-2ymhtDCMaa",
        "colab_type": "code",
        "outputId": "c8ab0678-cbdf-4b56-a0aa-0bd87e329c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Datasets/imdb_master.csv', encoding='latin-1')\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>neg</td>\n",
              "      <td>0_2.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10000_4.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10001_1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10002_3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test</td>\n",
              "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10003_3.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                             review label         file\n",
              "0  test  Once again Mr. Costner has dragged out a movie...   neg      0_2.txt\n",
              "1  test  This is an example of why the majority of acti...   neg  10000_4.txt\n",
              "2  test  First of all I hate those moronic rappers, who...   neg  10001_1.txt\n",
              "3  test  Not even the Beatles could write songs everyon...   neg  10002_3.txt\n",
              "4  test  Brass pictures (movies is not a fitting word f...   neg  10003_3.txt"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpc7fUFa5agq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(df[df['label'] == 'unsup'].index, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DQbcg6uCMah",
        "colab_type": "code",
        "outputId": "74965a87-36d9-4dcf-9bbb-566a6b644e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df['label'].unique()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neg', 'pos'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9RbXAknCMam",
        "colab_type": "code",
        "outputId": "76f3c210-ade5-44a6-eb46-fde9a8aaea5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "df['label'].unique()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHf67kAaCMar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = df[df['type'] == 'train'].drop(columns=['type','file']).values\n",
        "test = df[df['type'] == 'test'].drop(columns=['type','file']).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQLpCNiBCMaw",
        "colab_type": "code",
        "outputId": "d790c15d-4e7e-4735-b495-01a68dc89864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Length of Train: {}'.format(len(train)))\n",
        "print('Length of Test: {}'.format(len(test)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Train: 25000\n",
            "Length of Test: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_zIkysQCMa1",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mu6b7DiMdId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(sentence):\n",
        "  # Tokenize Contents\n",
        "  contentsTokenized = nltk.tokenize.word_tokenize(sentence)\n",
        "\n",
        "  # Remove the stop_words\n",
        "  stop_word_set = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "  filteredContents_afterstop = []\n",
        "  for word in contentsTokenized:\n",
        "    if word not in stop_word_set:\n",
        "      filteredContents_afterstop.append(word)\n",
        "  \n",
        "  # Performing porterStemming\n",
        "  porterStemmer = nltk.stem.PorterStemmer()\n",
        "  filteredContents = [porterStemmer.stem(word) for word in filteredContents_afterstop]\n",
        "\n",
        "  # Remove Punctuations\n",
        "  excludePunctuation = set(string.punctuation)\n",
        "    \n",
        "  # manually add additional punctuation to remove\n",
        "  doubleSingleQuote = '\\'\\''\n",
        "  doubleDash = '--'\n",
        "  doubleTick = '``'\n",
        "\n",
        "  excludePunctuation.add(doubleSingleQuote)\n",
        "  excludePunctuation.add(doubleDash)\n",
        "  excludePunctuation.add(doubleTick)\n",
        "\n",
        "  filteredContents_afterpunc = []\n",
        "  for word in filteredContents_afterstop:\n",
        "    if word not in excludePunctuation:\n",
        "      filteredContents_afterpunc.append(word)\n",
        "\n",
        "  # Convert all to lower case\n",
        "  filteredContents_lower = [term.lower() for term in filteredContents_afterpunc]\n",
        "  return filteredContents_lower"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JETWaXhaMvFM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43184294-574e-433c-ba29-6308ac79b471"
      },
      "source": [
        "a = \"Hey, I am Aritra Roy Gosthipaty.\"\n",
        "b = preprocess(a)\n",
        "print(b)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hey', 'i', 'aritra', 'roy', 'gosthipaty']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_XlgamSCMa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# token is a list of set\n",
        "# each set consists of words in the a particular review\n",
        "\n",
        "token = list(map(lambda x: set(preprocess(x)),train[:,0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyk8xypOPUQ2",
        "colab_type": "code",
        "outputId": "45f1996e-8b9f-45ec-f728-4787742008ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(token))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2_-XT8qCMa7",
        "colab_type": "code",
        "outputId": "a106fbe2-a625-4722-d2bb-a3494f9c1693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Total vocabulary\n",
        "vocab = set()\n",
        "for sentence in token:\n",
        "    for word in sentence:\n",
        "        if(len(word)>0):\n",
        "            vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "\n",
        "print('Total vocabulary count: {}'.format(len(vocab)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total vocabulary count: 114763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1rndxPCCMbA",
        "colab_type": "code",
        "outputId": "c3c8e399-28f2-4d3e-8c4e-d48ca7f58075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# For each word in vocab there is an index\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word] = i\n",
        "print('Total word2index count: {}'.format(len(word2index)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total word2index count: 114763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l30p-SMBFzTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1S4s7-dCMbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_dataset is used to keep the values of each words in each sentence\n",
        "input_dataset = list()\n",
        "for sentence in token:\n",
        "    sent_indices = list()\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sent_indices.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(list(sent_indices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saaej-JgXJhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = input_dataset\n",
        "y_train = train[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsFRUbbeve00",
        "colab_type": "code",
        "outputId": "35ab6bdb-6ae0-4ac1-c04b-788db2c65982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Type of X_train: {}'.format(type(X_train)))\n",
        "print('Len of X_train: {}'.format(len(X_train)))\n",
        "print()\n",
        "print('Type of y_train: {}'.format(type(y_train)))\n",
        "print('Type of y_train: {}'.format(len(y_train)))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of X_train: <class 'list'>\n",
            "Len of X_train: 25000\n",
            "\n",
            "Type of y_train: <class 'numpy.ndarray'>\n",
            "Type of y_train: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_tQZ_PZwUOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKjFWSMMycMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkKLQFHIzJJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha,iteration = (0.01, 2)\n",
        "hidden_size = 1000\n",
        "\n",
        "weight_0_1 = np.random.random((len(vocab),hidden_size))\n",
        "weight_1_2 = np.random.random((hidden_size,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL_7G_gU286_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOBMN3ro1CNw",
        "colab_type": "code",
        "outputId": "37b5ec92-933b-4964-8ae8-9c25091e187d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for iter in range(5):\n",
        "  for i in range(len(X_train)):\n",
        "    x,y = X_train[i], y_train[i]\n",
        "    layer_1 = sigmoid(np.sum(weight_0_1[x],axis=0))\n",
        "    layer_2 = sigmoid(np.dot(layer_1,weight_1_2))\n",
        "\n",
        "    layer_2_delta = layer_2 - y\n",
        "    layer_1_delta = layer_2_delta.dot(weight_1_2.T)\n",
        "\n",
        "    weight_0_1[x] -= layer_1_delta * alpha\n",
        "    weight_1_2 -= np.outer(layer_1,layer_2_delta) * alpha\n",
        "\n",
        "    if(np.abs(layer_2_delta) < 0.5):\n",
        "      correct += 1\n",
        "    total += 1\n",
        "\n",
        "    if(total % 1000 == 0):\n",
        "      print('Progress: {} Training Accuracy: {}'.format(total, correct/total))\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: 1000 Training Accuracy: 0.948\n",
            "Progress: 2000 Training Accuracy: 0.974\n",
            "Progress: 3000 Training Accuracy: 0.9826666666666667\n",
            "Progress: 4000 Training Accuracy: 0.987\n",
            "Progress: 5000 Training Accuracy: 0.9896\n",
            "Progress: 6000 Training Accuracy: 0.9913333333333333\n",
            "Progress: 7000 Training Accuracy: 0.9925714285714285\n",
            "Progress: 8000 Training Accuracy: 0.9935\n",
            "Progress: 9000 Training Accuracy: 0.9942222222222222\n",
            "Progress: 10000 Training Accuracy: 0.9948\n",
            "Progress: 11000 Training Accuracy: 0.9952727272727273\n",
            "Progress: 12000 Training Accuracy: 0.9956666666666667\n",
            "Progress: 13000 Training Accuracy: 0.9958461538461538\n",
            "Progress: 14000 Training Accuracy: 0.9961428571428571\n",
            "Progress: 15000 Training Accuracy: 0.9964\n",
            "Progress: 16000 Training Accuracy: 0.996625\n",
            "Progress: 17000 Training Accuracy: 0.9968235294117647\n",
            "Progress: 18000 Training Accuracy: 0.997\n",
            "Progress: 19000 Training Accuracy: 0.9971578947368421\n",
            "Progress: 20000 Training Accuracy: 0.9973\n",
            "Progress: 21000 Training Accuracy: 0.9974285714285714\n",
            "Progress: 22000 Training Accuracy: 0.9975454545454545\n",
            "Progress: 23000 Training Accuracy: 0.9976521739130435\n",
            "Progress: 24000 Training Accuracy: 0.99775\n",
            "Progress: 25000 Training Accuracy: 0.99784\n",
            "Progress: 26000 Training Accuracy: 0.9978461538461538\n",
            "Progress: 27000 Training Accuracy: 0.9979259259259259\n",
            "Progress: 28000 Training Accuracy: 0.998\n",
            "Progress: 29000 Training Accuracy: 0.9980689655172413\n",
            "Progress: 30000 Training Accuracy: 0.9981333333333333\n",
            "Progress: 31000 Training Accuracy: 0.9981935483870967\n",
            "Progress: 32000 Training Accuracy: 0.99825\n",
            "Progress: 33000 Training Accuracy: 0.9983030303030302\n",
            "Progress: 34000 Training Accuracy: 0.9983529411764706\n",
            "Progress: 35000 Training Accuracy: 0.9984\n",
            "Progress: 36000 Training Accuracy: 0.9984444444444445\n",
            "Progress: 37000 Training Accuracy: 0.9984864864864865\n",
            "Progress: 38000 Training Accuracy: 0.9984736842105263\n",
            "Progress: 39000 Training Accuracy: 0.9985128205128205\n",
            "Progress: 40000 Training Accuracy: 0.99855\n",
            "Progress: 41000 Training Accuracy: 0.9985853658536585\n",
            "Progress: 42000 Training Accuracy: 0.9986190476190476\n",
            "Progress: 43000 Training Accuracy: 0.9986511627906977\n",
            "Progress: 44000 Training Accuracy: 0.9986818181818182\n",
            "Progress: 45000 Training Accuracy: 0.9987111111111111\n",
            "Progress: 46000 Training Accuracy: 0.9987391304347826\n",
            "Progress: 47000 Training Accuracy: 0.9987659574468085\n",
            "Progress: 48000 Training Accuracy: 0.9987916666666666\n",
            "Progress: 49000 Training Accuracy: 0.9988163265306123\n",
            "Progress: 50000 Training Accuracy: 0.99884\n",
            "Progress: 51000 Training Accuracy: 0.9988235294117647\n",
            "Progress: 52000 Training Accuracy: 0.9988461538461538\n",
            "Progress: 53000 Training Accuracy: 0.9988679245283019\n",
            "Progress: 54000 Training Accuracy: 0.9988888888888889\n",
            "Progress: 55000 Training Accuracy: 0.9989090909090909\n",
            "Progress: 56000 Training Accuracy: 0.9989285714285714\n",
            "Progress: 57000 Training Accuracy: 0.9989473684210526\n",
            "Progress: 58000 Training Accuracy: 0.9989655172413793\n",
            "Progress: 59000 Training Accuracy: 0.9989830508474576\n",
            "Progress: 60000 Training Accuracy: 0.999\n",
            "Progress: 61000 Training Accuracy: 0.9990163934426229\n",
            "Progress: 62000 Training Accuracy: 0.9990322580645161\n",
            "Progress: 63000 Training Accuracy: 0.999015873015873\n",
            "Progress: 64000 Training Accuracy: 0.99903125\n",
            "Progress: 65000 Training Accuracy: 0.9990461538461538\n",
            "Progress: 66000 Training Accuracy: 0.9990606060606061\n",
            "Progress: 67000 Training Accuracy: 0.9990746268656716\n",
            "Progress: 68000 Training Accuracy: 0.9990882352941176\n",
            "Progress: 69000 Training Accuracy: 0.9991014492753623\n",
            "Progress: 70000 Training Accuracy: 0.9991142857142857\n",
            "Progress: 71000 Training Accuracy: 0.9991267605633802\n",
            "Progress: 72000 Training Accuracy: 0.9991388888888889\n",
            "Progress: 73000 Training Accuracy: 0.9991506849315068\n",
            "Progress: 74000 Training Accuracy: 0.9991621621621621\n",
            "Progress: 75000 Training Accuracy: 0.9991733333333334\n",
            "Progress: 76000 Training Accuracy: 0.9991578947368421\n",
            "Progress: 77000 Training Accuracy: 0.9991688311688312\n",
            "Progress: 78000 Training Accuracy: 0.9991794871794871\n",
            "Progress: 79000 Training Accuracy: 0.9991898734177215\n",
            "Progress: 80000 Training Accuracy: 0.9992\n",
            "Progress: 81000 Training Accuracy: 0.9992098765432099\n",
            "Progress: 82000 Training Accuracy: 0.999219512195122\n",
            "Progress: 83000 Training Accuracy: 0.9992289156626506\n",
            "Progress: 84000 Training Accuracy: 0.9992380952380953\n",
            "Progress: 85000 Training Accuracy: 0.9992470588235294\n",
            "Progress: 86000 Training Accuracy: 0.9992558139534884\n",
            "Progress: 87000 Training Accuracy: 0.999264367816092\n",
            "Progress: 88000 Training Accuracy: 0.99925\n",
            "Progress: 89000 Training Accuracy: 0.9992584269662922\n",
            "Progress: 90000 Training Accuracy: 0.9992666666666666\n",
            "Progress: 91000 Training Accuracy: 0.9992747252747253\n",
            "Progress: 92000 Training Accuracy: 0.9992826086956522\n",
            "Progress: 93000 Training Accuracy: 0.9992903225806452\n",
            "Progress: 94000 Training Accuracy: 0.9992978723404256\n",
            "Progress: 95000 Training Accuracy: 0.9993052631578947\n",
            "Progress: 96000 Training Accuracy: 0.9993125\n",
            "Progress: 97000 Training Accuracy: 0.999319587628866\n",
            "Progress: 98000 Training Accuracy: 0.9993265306122449\n",
            "Progress: 99000 Training Accuracy: 0.9993333333333333\n",
            "Progress: 100000 Training Accuracy: 0.99934\n",
            "Progress: 101000 Training Accuracy: 0.9993267326732673\n",
            "Progress: 102000 Training Accuracy: 0.9993333333333333\n",
            "Progress: 103000 Training Accuracy: 0.9993398058252427\n",
            "Progress: 104000 Training Accuracy: 0.9993461538461539\n",
            "Progress: 105000 Training Accuracy: 0.999352380952381\n",
            "Progress: 106000 Training Accuracy: 0.9993584905660378\n",
            "Progress: 107000 Training Accuracy: 0.9993644859813084\n",
            "Progress: 108000 Training Accuracy: 0.9993703703703704\n",
            "Progress: 109000 Training Accuracy: 0.9993761467889908\n",
            "Progress: 110000 Training Accuracy: 0.9993818181818181\n",
            "Progress: 111000 Training Accuracy: 0.9993873873873874\n",
            "Progress: 112000 Training Accuracy: 0.9993928571428572\n",
            "Progress: 113000 Training Accuracy: 0.9993805309734514\n",
            "Progress: 114000 Training Accuracy: 0.9993859649122807\n",
            "Progress: 115000 Training Accuracy: 0.9993913043478261\n",
            "Progress: 116000 Training Accuracy: 0.9993965517241379\n",
            "Progress: 117000 Training Accuracy: 0.9994017094017094\n",
            "Progress: 118000 Training Accuracy: 0.999406779661017\n",
            "Progress: 119000 Training Accuracy: 0.9994117647058823\n",
            "Progress: 120000 Training Accuracy: 0.9994166666666666\n",
            "Progress: 121000 Training Accuracy: 0.9994214876033057\n",
            "Progress: 122000 Training Accuracy: 0.9994262295081967\n",
            "Progress: 123000 Training Accuracy: 0.999430894308943\n",
            "Progress: 124000 Training Accuracy: 0.9994354838709677\n",
            "Progress: 125000 Training Accuracy: 0.99944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcde0KtH72Nw",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gldskekE5-Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token = list(map(lambda x: set(preprocess(x)),test[:,0]))\n",
        "\n",
        "# input_dataset is used to keep the values of each words in each sentence\n",
        "input_dataset_test = list()\n",
        "for sentence in token:\n",
        "    sent_indices = list()\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sent_indices.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset_test.append(list(sent_indices))\n",
        "\n",
        "X_test = input_dataset_test\n",
        "y_test = test[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS2ef5mx_F5X",
        "colab_type": "code",
        "outputId": "1a92e319-b53f-4542-ff80-5dbdee52fd0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLeMT0IP87N9",
        "colab_type": "code",
        "outputId": "280169df-005c-40ab-8ddf-ab8c39e17229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "correct,total = 0,0\n",
        "for i in range(len(X_test)):\n",
        "    x,y = X_test[i], y_test[i]\n",
        "    layer_1 = sigmoid(np.sum(weight_0_1[x],axis=0))\n",
        "    layer_2 = sigmoid(np.dot(layer_1,weight_1_2))\n",
        "\n",
        "    layer_2_delta = layer_2 - y\n",
        "    layer_1_delta = layer_2_delta.dot(weight_1_2.T)\n",
        "\n",
        "    if(np.abs(layer_2_delta) < 0.5):\n",
        "      correct += 1\n",
        "    total += 1\n",
        "\n",
        "    if(total % 1000 == 0):\n",
        "      print('Progress: {} Training Accuracy: {}'.format(total, correct/total))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: 1000 Training Accuracy: 0.0\n",
            "Progress: 2000 Training Accuracy: 0.0\n",
            "Progress: 3000 Training Accuracy: 0.0\n",
            "Progress: 4000 Training Accuracy: 0.0\n",
            "Progress: 5000 Training Accuracy: 0.0\n",
            "Progress: 6000 Training Accuracy: 0.0\n",
            "Progress: 7000 Training Accuracy: 0.0\n",
            "Progress: 8000 Training Accuracy: 0.0\n",
            "Progress: 9000 Training Accuracy: 0.0\n",
            "Progress: 10000 Training Accuracy: 0.0\n",
            "Progress: 11000 Training Accuracy: 0.0\n",
            "Progress: 12000 Training Accuracy: 0.0\n",
            "Progress: 13000 Training Accuracy: 0.038461538461538464\n",
            "Progress: 14000 Training Accuracy: 0.10714285714285714\n",
            "Progress: 15000 Training Accuracy: 0.16666666666666666\n",
            "Progress: 16000 Training Accuracy: 0.21875\n",
            "Progress: 17000 Training Accuracy: 0.2647058823529412\n",
            "Progress: 18000 Training Accuracy: 0.3055555555555556\n",
            "Progress: 19000 Training Accuracy: 0.34210526315789475\n",
            "Progress: 20000 Training Accuracy: 0.375\n",
            "Progress: 21000 Training Accuracy: 0.40476190476190477\n",
            "Progress: 22000 Training Accuracy: 0.4318181818181818\n",
            "Progress: 23000 Training Accuracy: 0.45652173913043476\n",
            "Progress: 24000 Training Accuracy: 0.4791666666666667\n",
            "Progress: 25000 Training Accuracy: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}